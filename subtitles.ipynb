{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgnoob/subtitles/blob/main/subtitles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oADWIoi9200s"
      },
      "source": [
        "# How to use\n",
        "* https://github.com/sgnoob/subtitles?tab=readme-ov-file#how-to-use-the-colab-file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N1ZRLugK75D"
      },
      "source": [
        "# Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LQM2A9NJ1rc9"
      },
      "outputs": [],
      "source": [
        "# @markdown # Input\n",
        "INPUT_SOURCE = \"yt-dlp\" # @param [\"yt-dlp\", \"GoogleDrive\"]\n",
        "\n",
        "# @markdown ### YouTube Only\n",
        "VIDEO_URL ='https://youtu.be/eXDGNKKoxZs' #@param {type:\"string\"}\n",
        "COPY_VIDEO_TO_DRIVE = True # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ### Google Drive Only\n",
        "# @markdown Please don't use names with special characters such as space.\n",
        "INPUT_FILE = \"input/test.mp4\" #@param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown # Output\n",
        "OUTPUT_DIR = 'output' #@param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown # Mel-Band-Roformer Settings\n",
        "MEL_SEGMENT_TIME = 3600 #@param {type:\"integer\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown # Whisper Settings\n",
        "MODEL = 'large-v2' #@param {type:\"string\"}\n",
        "VAD_METHOD = 'pyannote_v3' #@param [\"silero_v4_fw\", \"silero_v5_fw\", \"silero_v3\", \"silero_v4\", \"silero_v5\", \"pyannote_v3\", \"pyannote_onnx_v3\", \"auditok\", \"webrtc\"]\n",
        "VAD_THRESHOLD = 0.1 #@param {type:\"number\"}\n",
        "# VAD_THRESHOLD default is 0.45\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown # Gemini SRT Translator Settings\n",
        "GEMINI_API_KEY_1 = 'NONE' #@param {type:\"string\"}\n",
        "GEMINI_API_KEY_2 = 'NONE' #@param {type:\"string\"}\n",
        "GEMINI_MODEL = 'gemini-2.5-flash' #@param {type:\"string\"}\n",
        "GEMINI_THINKING = True # @param {type:\"boolean\"}\n",
        "GEMINI_THINKING_BUDGET = 24576 # @param {type:\"slider\", min:0, max:24576, step:1}\n",
        "USE_AUDIO_CONTEXT = False # @param {type:\"boolean\"}\n",
        "\n",
        "from google.colab import userdata\n",
        "try:\n",
        "  GEMINI_API_KEY_1 = userdata.get('GEMINI_API_KEY_1')\n",
        "  print(\"Obtained GEMINI_API_KEY_1 from Colab Secrets.\")\n",
        "except:\n",
        "  pass\n",
        "try:\n",
        "  GEMINI_API_KEY_2 = userdata.get('GEMINI_API_KEY_2')\n",
        "  print(\"Obtained GEMINI_API_KEY_2 from Colab Secrets.\")\n",
        "except:\n",
        "  pass\n",
        "\n",
        "GEMINI_API_KEY_1= GEMINI_API_KEY_1.strip()\n",
        "GEMINI_API_KEY_2= GEMINI_API_KEY_2.strip()\n",
        "\n",
        "if len(GEMINI_API_KEY_1) == 0 or GEMINI_API_KEY_1.lower() == \"none\":\n",
        "  GEMINI_API_KEY_1 = None\n",
        "if len(GEMINI_API_KEY_2) == 0 or GEMINI_API_KEY_2.lower() == \"none\":\n",
        "  GEMINI_API_KEY_2 = None\n",
        "\n",
        "if GEMINI_API_KEY_1 is None:\n",
        "  print(\"ERROR. GEMINI_API_KEY_1 is empty. How are we going to translate?\")\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown # Others\n",
        "DELETE_INTERMEDIATE_FILES = True # @param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "# Internal Use only\n",
        "MOUNT_DIR = '/content/drive'\n",
        "INPUT_FILE = f\"{MOUNT_DIR}/MyDrive/{INPUT_FILE}\"\n",
        "OUTPUT_DIR = f\"{MOUNT_DIR}/MyDrive/{OUTPUT_DIR}\"\n",
        "\n",
        "EXEC_DIR = '/content/exec'\n",
        "TMP_DIR = '/content/tmp'\n",
        "! mkdir -p {EXEC_DIR}\n",
        "if DELETE_INTERMEDIATE_FILES:\n",
        "  ! rm -rf {TMP_DIR}\n",
        "! mkdir -p {TMP_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpIhh9hzA_QX"
      },
      "source": [
        "# Mount Google Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSwLGFJh5RbP"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(MOUNT_DIR)\n",
        "\n",
        "! mkdir -p {OUTPUT_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLbQwx5n6WgZ"
      },
      "source": [
        "# Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5CfXW9b6ZVz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import base64\n",
        "\n",
        "os.environ['MPLBACKEND'] = 'Agg' # Copied from https://github.com/Purfview/whisper-standalone-win/discussions/385\n",
        "\n",
        "! apt-get install -y aria2\n",
        "! apt-get install -y ffmpeg\n",
        "! pip3 install -U gemini-srt-translator pysubs2 yt-dlp pydub\n",
        "\n",
        "# Download Faster-Whisper-XXL\n",
        "# I find the latest 7zip to be slightly faster in decompressing compared to apt-get\n",
        "if not os.path.exists(f'{EXEC_DIR}/Faster-Whisper-XXL/faster-whisper-xxl'):\n",
        "  ! cd {EXEC_DIR}; wget -N https://www.7-zip.org/a/7z2409-linux-x64.tar.xz\n",
        "  ! cd {EXEC_DIR}; tar -xvf 7z2409-linux-x64.tar.xz 7zz\n",
        "  ! cd {EXEC_DIR}; aria2c -x4 -s4 https://github.com/Purfview/whisper-standalone-win/releases/download/Faster-Whisper-XXL/Faster-Whisper-XXL_r245.4_linux.7z -o whisper.7z\n",
        "  ! cd {EXEC_DIR}; ./7zz x whisper.7z -aoa\n",
        "  ! chmod +x {EXEC_DIR}/Faster-Whisper-XXL/faster-whisper-xxl\n",
        "else:\n",
        "  print('Faster-Whisper-XXL/faster-whisper-xxl is already extracted, not extracting again.')\n",
        "\n",
        "# Setup Music-Source-Separation-Training (to extract vocals)\n",
        "! cd {EXEC_DIR}; git clone https://github.com/sgnoob/Mel-Band-Roformer-Vocal-Model.git\n",
        "! cd {EXEC_DIR}; pip3 install -r Mel-Band-Roformer-Vocal-Model/requirements.txt\n",
        "if not os.path.exists(f'{EXEC_DIR}/MelBandRoformer.ckpt'):\n",
        "  ! cd {EXEC_DIR}; aria2c -x4 -s4 https://huggingface.co/KimberleyJSN/melbandroformer/resolve/main/MelBandRoformer.ckpt -o MelBandRoformer.ckpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksT1ZWx4MS_J"
      },
      "source": [
        "# Download video using yt-dlp if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obbFz2Y-MkwE"
      },
      "outputs": [],
      "source": [
        "if INPUT_SOURCE == \"yt-dlp\":\n",
        "  ! yt-dlp --list-formats --extractor-arg \"youtube:player_client=default,android_vr\" \"{VIDEO_URL}\"\n",
        "  RAW = ! yt-dlp --extractor-arg \"youtube:player_client=default,android_vr\" -f \"bv+ba/b\" -o \"%(id)s.%(ext)s\" \"{VIDEO_URL}\" --print filename --skip-download\n",
        "  YTDLP_FILENAME = RAW[0]\n",
        "  ! cd {TMP_DIR}; yt-dlp --extractor-arg \"youtube:player_client=default,android_vr\" -f \"bv+ba/b\" -o \"%(id)s.%(ext)s\" \"{VIDEO_URL}\"\n",
        "  INPUT_FILE = f\"{TMP_DIR}/{YTDLP_FILENAME}\"\n",
        "  ! ls -lRt {TMP_DIR}\n",
        "\n",
        "  if COPY_VIDEO_TO_DRIVE:\n",
        "    ! cp \"{INPUT_FILE}\" \"{OUTPUT_DIR}/.\"\n",
        "else:\n",
        "  print(f\"No need to download using yt-dlp. INPUT_SOURCE: {INPUT_SOURCE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uBOLVS9k9UZ"
      },
      "source": [
        "## Sort out filenames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPBVsy51k_2O"
      },
      "outputs": [],
      "source": [
        "# Sort out filenames\n",
        "from pathlib import Path\n",
        "p = Path(INPUT_FILE)\n",
        "FILENAME_STEM = f\"{p.stem}\"\n",
        "\n",
        "MEL_VOCALS_FILE = f\"{TMP_DIR}/vocals.wav\"\n",
        "\n",
        "OUT_EN_FILE_BASE = f\"{FILENAME_STEM}.en.srt\"\n",
        "OUT_EN_FILE = f\"{OUTPUT_DIR}/{OUT_EN_FILE_BASE}\"\n",
        "OUT_JP_FILE_BASE = f\"{FILENAME_STEM}.jp.srt\"\n",
        "OUT_JP_FILE = f\"{OUTPUT_DIR}/{OUT_JP_FILE_BASE}\"\n",
        "OUT_EN_JP_FILE = f\"{OUTPUT_DIR}/{FILENAME_STEM}.en_jp.ass\"\n",
        "\n",
        "print(f\"TMP_DIR: {TMP_DIR}\")\n",
        "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")\n",
        "print(f\"FILENAME_STEM: {FILENAME_STEM}\")\n",
        "print('\\n')\n",
        "print(f\"MEL_VOCALS_FILE: {MEL_VOCALS_FILE}\")\n",
        "print('\\n')\n",
        "print(f\"OUT_EN_FILE: {OUT_EN_FILE}\")\n",
        "print(f\"OUT_JP_FILE: {OUT_JP_FILE}\")\n",
        "print(f\"OUT_EN_JP_FILE: {OUT_EN_JP_FILE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpQmvVjH_PhO"
      },
      "source": [
        "# Extract vocals from input for better transcription\n",
        "\n",
        "Using Mel-Band-Roformer for this.\n",
        "* https://github.com/KimberleyJensen/Mel-Band-Roformer-Vocal-Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7zH2c4U_bgW"
      },
      "outputs": [],
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "def find_best_split_point(audio, max_time_ms, search_window_ms=60000, analysis_window_ms=100):\n",
        "    \"\"\"\n",
        "    Find the best split point by searching backwards from max_time_ms for the quietest moment.\n",
        "    \"\"\"\n",
        "    # Define search range - search backwards from target time\n",
        "    start_search = max(0, max_time_ms - search_window_ms)\n",
        "    end_search = min(len(audio), max_time_ms)\n",
        "    \n",
        "    if end_search - start_search < analysis_window_ms:\n",
        "        return max_time_ms\n",
        "    \n",
        "    # Find the quietest moment by analyzing the audio in chunks\n",
        "    best_quietness = float('inf')\n",
        "    best_position = max_time_ms\n",
        "    \n",
        "    # Step through backwards, analyzing chunks of analysis_window_ms\n",
        "    for pos in range(end_search - analysis_window_ms, start_search - 1, -analysis_window_ms):\n",
        "        # Get segment to analyze\n",
        "        test_segment = audio[pos:pos + analysis_window_ms]\n",
        "        \n",
        "        if len(test_segment) == analysis_window_ms:\n",
        "            # Calculate RMS (Root Mean Square) for better quietness detection\n",
        "            # RMS is more reliable than dBFS for finding quiet moments\n",
        "            samples = test_segment.get_array_of_samples()\n",
        "            if len(samples) > 0:\n",
        "                rms = (sum(s**2 for s in samples) / len(samples)) ** 0.5\n",
        "                \n",
        "                # Lower RMS = quieter\n",
        "                if rms < best_quietness:\n",
        "                    best_quietness = rms\n",
        "                    best_position = pos + (analysis_window_ms // 2)\n",
        "    \n",
        "    return best_position\n",
        "\n",
        "def smart_split_audio(input_file, output_dir, segment_time_seconds=3600, search_window_ms=60000, analysis_window_ms=100):\n",
        "    \"\"\"\n",
        "    Split audio file into segments at silence gaps using pydub, then convert to required format.\n",
        "    \"\"\"\n",
        "    print(f\"Loading audio file for smart splitting: {input_file}\")\n",
        "    \n",
        "    # Load audio file\n",
        "    audio = AudioSegment.from_file(input_file)\n",
        "    \n",
        "    # Calculate segment parameters\n",
        "    segment_time_ms = segment_time_seconds * 1000\n",
        "    total_duration_ms = len(audio)\n",
        "    \n",
        "    print(f\"Total duration: {total_duration_ms / 1000:.2f} seconds\")\n",
        "    print(f\"Target segment length: {segment_time_seconds} seconds\")\n",
        "    \n",
        "    # Split the audio\n",
        "    current_pos = 0\n",
        "    segment_num = 0\n",
        "    \n",
        "    while current_pos < total_duration_ms:\n",
        "        # Calculate target end time for this segment\n",
        "        target_end = current_pos + segment_time_ms\n",
        "        \n",
        "        if target_end >= total_duration_ms:\n",
        "            # Last segment - take everything remaining\n",
        "            actual_end = total_duration_ms\n",
        "        else:\n",
        "            # Find best split point before target (treat target as maximum)\n",
        "            actual_end = find_best_split_point(\n",
        "                audio, target_end, \n",
        "                search_window_ms=search_window_ms,\n",
        "                analysis_window_ms=analysis_window_ms\n",
        "            )\n",
        "        \n",
        "        # Extract segment\n",
        "        segment = audio[current_pos:actual_end]\n",
        "        segment_duration = (actual_end - current_pos) / 1000\n",
        "        \n",
        "        # Generate output filename (matching the original ffmpeg format)\n",
        "        output_file = os.path.join(output_dir, f\"part{segment_num:03d}.wav\")\n",
        "        \n",
        "        # Export segment with the required format (44100 Hz, 16-bit, stereo)\n",
        "        segment = segment.set_frame_rate(44100).set_channels(2).set_sample_width(2)\n",
        "        segment.export(output_file, format=\"wav\")\n",
        "        \n",
        "        print(f\"Segment {segment_num}: {segment_duration:.2f}s -> part{segment_num:03d}.wav\")\n",
        "        \n",
        "        current_pos = actual_end\n",
        "        segment_num += 1\n",
        "    \n",
        "    print(f\"Smart splitting complete! Created {segment_num} segments.\")\n",
        "    return segment_num\n",
        "\n",
        "# Your existing code with smart splitting integration\n",
        "if os.path.exists(OUT_JP_FILE):\n",
        "  print(f\"{OUT_JP_FILE_BASE} already exists, not re-running Mel-Band-Roformer.\")\n",
        "  print(f\"Delete {OUT_JP_FILE_BASE} from your google drive if you want to re-run.\")\n",
        "elif os.path.exists(MEL_VOCALS_FILE):\n",
        "  print(f\"{MEL_VOCALS_FILE} already exists, not re-running Mel-Band-Roformer\")\n",
        "else:\n",
        "  # Smart split the input audio into chunks to prevent Out of Memory\n",
        "  # This will split at silence gaps to avoid cutting words in half\n",
        "  print(f'Using smart splitting to split audio into ~{MEL_SEGMENT_TIME} seconds chunks')\n",
        "  \n",
        "  try:\n",
        "    # Try smart splitting first\n",
        "    num_segments = smart_split_audio(\n",
        "        INPUT_FILE, \n",
        "        TMP_DIR, \n",
        "        segment_time_seconds=MEL_SEGMENT_TIME,\n",
        "        search_window_ms=60000,  # Default 60s search window for split point\n",
        "        analysis_window_ms=100   # Analyze 100ms chunks for quietness\n",
        "    )\n",
        "    print(f\"Smart splitting successful! Created {num_segments} segments.\")\n",
        "    \n",
        "  except Exception as e:\n",
        "    print(f\"Smart splitting failed ({e}), falling back to ffmpeg time-based splitting...\")\n",
        "    # Fallback to original ffmpeg method\n",
        "    command = f\"cd {TMP_DIR}; ffmpeg\"\n",
        "    command += f\" -n -i '{INPUT_FILE}'\"\n",
        "    command += f\" -vn -af 'aresample=44100' -acodec pcm_s16le -ac 2\"\n",
        "    command += f\" -f segment -segment_time {MEL_SEGMENT_TIME} -reset_timestamps 1\"\n",
        "    command += f\" part%03d.wav\"\n",
        "    print(f\"Fallback command: {command}\")\n",
        "    ! {command}\n",
        "\n",
        "  # Run Mel-Band-Roformer to extract vocals\n",
        "  print('Running Mel-Band-Roformer to extract vocals')\n",
        "  command = f\"cd {EXEC_DIR}/Mel-Band-Roformer-Vocal-Model; python inference.py\"\n",
        "  command += f\" --model_type mel_band_roformer\"\n",
        "  command += f\" --config_path configs/config_vocals_mel_band_roformer.yaml\"\n",
        "  command += f\" --model_path {EXEC_DIR}/MelBandRoformer.ckpt\"\n",
        "  command += f\" --input_folder {TMP_DIR}\"\n",
        "  command += f\" --store_dir {TMP_DIR}\"\n",
        "  print(f\"Command: {command}\")\n",
        "  ! {command}\n",
        "\n",
        "  print(f'Using ffmpeg to recombine vocal chunks')\n",
        "  ! cd {TMP_DIR}; printf \"file '%s'\\n\" part*_vocals.wav | sort -V > mylist.txt\n",
        "  command = f\"cd {TMP_DIR}; ffmpeg\"\n",
        "  command += f\" -n -f concat -safe 0 -i mylist.txt -ar 16000 -acodec pcm_s16le {MEL_VOCALS_FILE}\"\n",
        "  print(f\"Command: {command}\")\n",
        "  ! {command}\n",
        "  ! ls -lRt {TMP_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CHPcBOwOPmV"
      },
      "source": [
        "# Generate Transcription\n",
        "* Using https://github.com/Purfview/whisper-standalone-win"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIOF0qu4ONn3"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(f\"{OUT_JP_FILE}\"):\n",
        "  print(f\"{OUT_JP_FILE_BASE} already exists, not re-running whisper transcription.\")\n",
        "  print(f\"Delete {OUT_JP_FILE_BASE} from your google drive if you want to re-run.\")\n",
        "else:\n",
        "  print('Preparing whisper command')\n",
        "  # Process each vocal part file\n",
        "  part_files = sorted([f for f in os.listdir(TMP_DIR) if f.startswith('part') and f.endswith('_vocals.wav')])\n",
        "  total_parts = len(part_files)\n",
        "  print(f\"Total parts to process: {total_parts}\")\n",
        "\n",
        "  for i, part_file in enumerate(part_files):\n",
        "      print(f\"Processing part {i+1}/{total_parts}: {part_file}\")\n",
        "      part_input_file = os.path.join(TMP_DIR, part_file)\n",
        "\n",
        "      command = f\"{EXEC_DIR}/Faster-Whisper-XXL/faster-whisper-xxl\"\n",
        "      command += f\" --model {MODEL}\"\n",
        "      command += f\" --language ja\"\n",
        "      command += f\" --vad_method {VAD_METHOD}\"\n",
        "      command += f\" --vad_threshold {VAD_THRESHOLD}\"\n",
        "      command += f\" --task transcribe\"\n",
        "      command += f\" --output_format srt\"\n",
        "      command += f\" --output_dir \\\"{TMP_DIR}\\\"\"\n",
        "      command += f\" -- \\\"{part_input_file}\\\"\"\n",
        "      print(f\"Command: {command}\")\n",
        "      ! {command}\n",
        "\n",
        "  # Merge the generated SRT files\n",
        "  print('Merging SRT files')\n",
        "  merged_srt_file = os.path.join(TMP_DIR, \"merged.srt\")\n",
        "  with open(merged_srt_file, 'w') as outfile:\n",
        "      for part_file in part_files:\n",
        "          part_srt_file = os.path.join(TMP_DIR, part_file.replace('.wav', '.srt'))\n",
        "          if os.path.exists(part_srt_file):\n",
        "              with open(part_srt_file, 'r') as infile:\n",
        "                  outfile.write(infile.read())\n",
        "              # Add a newline between files if needed (adjust based on desired output)\n",
        "              # outfile.write('\\n')\n",
        "          else:\n",
        "              print(f\"WARNING: SRT file not found for {part_file}.\")\n",
        "\n",
        "  # Copy the merged SRT to the output directory\n",
        "  ! cp {merged_srt_file} {OUTPUT_DIR}/{FILENAME_STEM}.jp.srt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-0_hMZMONV5"
      },
      "source": [
        "# Translate to English\n",
        "* Using https://github.com/MaKTaiL/gemini-srt-translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfNCSqOCzlXu"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(OUT_EN_FILE):\n",
        "  print(f\"{OUT_EN_FILE_BASE} already exists, not re-running gemini-srt-translator.\")\n",
        "  print(f\"Delete {OUT_EN_FILE_BASE} from your google drive if you want to re-run.\")\n",
        "else:\n",
        "  import gemini_srt_translator as gst\n",
        "  gst.gemini_api_key = GEMINI_API_KEY_1\n",
        "  gst.gemini_api_key2 = GEMINI_API_KEY_2\n",
        "  gst.model_name = GEMINI_MODEL\n",
        "  gst.target_language = \"English\"\n",
        "  gst.input_file = OUT_JP_FILE\n",
        "  gst.output_file = OUT_EN_FILE\n",
        "  gst.thinking = GEMINI_THINKING\n",
        "  gst.thinking_budget = GEMINI_THINKING_BUDGET\n",
        "  gst.progress_log = True\n",
        "  gst.thoughts_log = True\n",
        "  if USE_AUDIO_CONTEXT:\n",
        "    gst.audio_file = MEL_VOCALS_FILE\n",
        "  gst.translate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOJLsm4OkpXg"
      },
      "source": [
        "## Combine Japanese and English subtitles into a single .ass file\n",
        "JP subtitles on top, EN subtitles below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKJ79ll_j4LD"
      },
      "outputs": [],
      "source": [
        "# Combine the EN and JP subtitles into an EN_JP ASS file.\n",
        "# JP subtitles on top, EN subtitles below.\n",
        "if os.path.exists(OUT_JP_FILE) and os.path.exists(OUT_EN_FILE):\n",
        "  import pysubs2\n",
        "  subs_jp = pysubs2.load(OUT_JP_FILE)\n",
        "  subs_en = pysubs2.load(OUT_EN_FILE)\n",
        "  subs_en_jp = pysubs2.SSAFile()\n",
        "  subs_en_jp.styles = {\n",
        "    \"b\": pysubs2.SSAStyle(alignment=pysubs2.Alignment.BOTTOM_CENTER),\n",
        "    \"t\": pysubs2.SSAStyle(alignment=pysubs2.Alignment.TOP_CENTER),\n",
        "  }\n",
        "  for e in subs_en:\n",
        "      e.style = \"b\"\n",
        "      subs_en_jp.append(e)\n",
        "  for e in subs_jp:\n",
        "      e.style = \"t\"\n",
        "      subs_en_jp.append(e)\n",
        "\n",
        "  subs_en_jp.save(OUT_EN_JP_FILE)\n",
        "\n",
        "print(\"DONE!!!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM1OLwk7sd/XDhiQw/sQGAi",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
